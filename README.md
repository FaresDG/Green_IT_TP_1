# ğŸ“Š ComparIA â€“ LLM Benchmark Dashboard

**ComparIA** is an interactive **Streamlit dashboard** designed to benchmark and visualize the performance of multiple **Large Language Models (LLMs)** across diverse categories of tasks.  
It combines **quality**, **latency**, **energy consumption**, and **COâ‚‚ emissions** to analyze trade-offs between **performance and efficiency**.

---

## ğŸ§  Overview

This project allows you to:

- ğŸ“ˆ **Compare** LLMs by size and task category  
- âš™ï¸ **Explore** relationships between metrics (Quality vs Energy, Quality vs COâ‚‚, Latency per model size)  
- ğŸ **Rank** models per use case using **custom weights** (Quality, Latency, Energy)  
- ğŸ” **Filter** data dynamically by model size, model name, and task type  

All charts are **interactive** and generated with **Plotly**.

---

## ğŸ§± Project Structure

LLM_BENCHMARCK_DASHBOARD/
â”‚
â”œâ”€â”€ .streamlit/
â”‚ â””â”€â”€ config.toml # Optional Streamlit theme configuration
â”‚
â”œâ”€â”€ app/
â”‚ â””â”€â”€ data/
â”‚ â””â”€â”€ data.csv # Main dataset (LLM benchmark results)
â”‚
â”œâ”€â”€ pages/
â”‚ â”œâ”€â”€ App.py # Aggregated view (Quality, Energy, COâ‚‚, Latency)
â”‚ â”œâ”€â”€ Exploration.py # Metric exploration (line & bar charts)
â”‚ â””â”€â”€ Ranking.py # Ranking page (weighted scoring system)
â”‚
â”œâ”€â”€ README.md # Documentation (this file)
â”œâ”€â”€ requirements.txt # Dependencies
â””â”€â”€ .gitignore


---

## âš™ï¸ Installation

1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/LLM_BENCHMARCK_DASHBOARD.git
cd LLM_BENCHMARCK_DASHBOARD

2ï¸âƒ£ Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate     # macOS / Linux
.venv\Scripts\activate        # Windows

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt


ğŸš€ Running the app

Launch the dashboard:
streamlit run pages/App.py

ğŸ“Š Dataset Schema
Column	Description
run_id	Unique identifier for each benchmark run
task_id	Numeric ID of the evaluated task
task_label	Text describing the prompt or task
model	Model name (e.g. GPT-5, Gemma 8B)
model_size	Size category (Small / Medium / Large)
quality	Quality score (1â€“5)
latency_s	Average response time (seconds)
energy_kwh	Energy consumption per run (Wh)
co2_kg	COâ‚‚ emissions per run (g eq.)


ğŸ§© Key Pages

ğŸ§­ App.py â€“ Aggregated Dashboard
Bar charts comparing:
Average Quality, Latency, Energy, and COâ‚‚
By Model Size and Task Category

ğŸ”¬ Exploration.py â€“ Metrics Exploration
Line and bar charts exploring:
Quality vs Energy
Quality vs COâ‚‚ Emissions
Average Latency per Model Size

ğŸ Ranking.py â€“ Model Ranking
Ranks models per task category using weighted scoring:
Adjustable sliders for Quality, Latency, and Energy
Interactive Top 5 bar chart per use case
CSV export of the complete ranking


ğŸ§  How the Weighted Ranking Works
Each model gets a normalized score per metric:
Higher is better for Quality
Lower is better for Latency and Energy
The final score is computed as:   Score=wQâ€‹Ã—Q+wLâ€‹Ã—(1âˆ’L)+wEâ€‹Ã—(1âˆ’E),

where ( w_Q ), ( w_L ), and ( w_E ) are user-defined weights (whose total does not exceed 1), and all metrics are normalized between 0 and 1 prior to aggregation.

ğŸ“¦ Dependencies
Library	Purpose
streamlit	Web app interface
pandas	Data processing
plotly	Interactive visualizations
numpy	Numerical normalization
pathlib	File system utilities

ğŸ§‘â€ğŸ’» Author

Marie-Pierre DIQUERO & Fares DOSSOGBETE 
MSc Data Engineering & Cloud Computing @ Aivancity Paris
